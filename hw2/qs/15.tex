Let $S = \left\{u_1,u_2,\dots,u_n\right\}$ be a finite set of
vectors. Prove that $S$ is linearly dependent if and only if $u_1 =0$
or $u_{k+1} \in
\text{span}\left(\left\{u_1,u_2,\dots,u_k\right\}\right)$ for some $k
\left(1 \leq k < n\right)$.
\\\hfill\\
\textbf{Forward Direction:}\\
% Claim: $S$ is linearly dependent 
% \\Suppose $u_1=0$
% \\Assume $S$ is linearly independent.
% \\Take the linear combination:
% \begin{equation}
% c_1u_1 + c_2u_2 + \cdots + c_nu_n =0 \; \;\text{such that}\; c_1 \neq 0,
% c_2,\dots,c_n = 0
% \end{equation}
% \begin{equation}
% \implies c_1u_1 = 0 \;\lightning\; \text{Contradiction!}
% \end{equation}
% There exists a non-trivial representation of the zero vector therefore
% $S$ is linearly dependent.\\\hfill
% \\Suppose $u_{k+1} \in \text{span}(\left\{u_1,\dots,u_k\right\})$ 
% \\Assume $S$ is linearly independent.
% \begin{equation}
% u_{k+1} = a_1u_1 + a_2u_2 + \cdots + a_ku_k
% \end{equation}
% Take the linearly combination:
% \begin{equation}
% c_1u_1 + c_2u_2 + \cdots + c_ku_k + c_{k+1} + \dots + c_nu_n = 0
% \end{equation}
% Choose $c_i = (-a_i)$ for $i(1\leq i \leq k)$ and $c_{k+1}=1$
% \begin{equation}
% \implies u_{k+1} = 0 \;\lightning\; \text{Contradiction!}
% \end{equation}
% There exists a non-trivial representation of the zero vector therefore
% $S$ is linearly dependent.
\\Suppose $S$ is linearly dependent.
\\Claim $u_1 =0$
or $u_{k+1} \in
\text{span}\left(\left\{u_1,u_2,\dots,u_k\right\}\right)$ for some $k
\left(1 \leq k < n\right)$.
\\Let $a_1u_1 +a_2u_2 +\cdots+a_{k+1}u_{k+1}=0,\; a_i \in F$
\paragraph{Case 1} $\exists k$ such that $a_1u_1 +a_2u_2
+\cdots+a_ku_k+a_{k+1}u_{k+1}=0,\;a_{k+1} \neq 0$
\begin{equation}
\implies \frac{-a_1}{a_{k+1}}u_1 + \frac{-a_2}{a_{k+1}} +\cdots +
\frac{a_k}{a_{k+1}}u_k = u_{k+1}
\end{equation}
\begin{equation}
\implies u_{k+1} \in \text{span}(\{u_1,\dots,u_k\})
\end{equation}
\paragraph{Case 2} $\nexists k$ such that $a_1u_1 +a_2u_2
+\dots+a_ku_k+a_{k+1}u_{k+1}=0,\; a_{k+1}\neq 0$
\begin{equation}
\implies a_{k+1}=0 \; \forall k(1 \leq k< n) 
\end{equation}
\begin{equation}
\implies a_2=a_3=\cdots=a_{k+1}=0\;\forall k (1\leq k < n)
\end{equation}
\begin{equation}
%\implies a_1u_1 +a_2u_2
%+\dots+a_ku_k+a_{k+1}u_{k+1}&= a_1u_1 + 0\cdot u_2+\cdots
%+0\cdot u_{k+1} \\
\implies a_1u_1 = 0
\end{equation}
Because $S$ is linearly dependent $a_1\neq 0$ 
\begin{equation}
\implies  u_1 = 0
\end{equation}
\\
\textbf{Reverse Direction:}\\
Suppose $u_{k+1} \in \text{span}(\left\{u_1,\dots,u_k\right\})$
\\Claim: $S$ is linearly dependent 
\begin{align}
u_{k+1}  &= a_1u_1 + a_2u_2 + \cdots + a_ku_k\\
-u_{k+1} &= \left(-1\right)\left(a_1u_1 + a_2u_2 + \cdots +
  a_ku_k\right)\\
&= \left(-a_1\right)u_1 + \left(a_2\right)u_2 + \cdots + \left(-a_k\right)u_k
\end{align}
Take the linear combination of all $u_1,\dots,u_n$
\begin{equation}
\left(\left(-a_1\right)u_1 + \left(a_2\right)u_2 + \cdots +
  \left(-a_k\right)u_k\right) + \left(1u_{k+1} + 0u_{k+2}
  + \cdots + 0u_n\right)=0
\end{equation}
Suppose $u_1 = 0$
\\Claim: $S$ is linearly dependent
\\Let:
\begin{align}
a_1u_1 +a_2u_n +\cdots+a_nu_n &= 0,\; \forall a_i \in F\\
\text{such that}\;a_1u_1 + 0\cdot u_2 +\cdots+0\cdot u_n &= 0,\;
a_1\neq 0\\
a_1u_1 &= 0
\end{align}
